# Towards Using Virtual Acoustics for Evaluating Spatial Ecoacoustic Monitoring Technologies
Analysis scripts for the validation and pilot of a Virtual Sound Environment (VSE) for testing (spatial) ecoacoustic technologies.

## Context
This repository contains the analysis scripts used for the project ‘Towards Using Virtual Acoustics for Evaluating Spatial Ecoacoustic Monitoring Technologies’. In this work, we developed an ambisonic Virtual Sound Environment (VSE) for simulating real natural soundscapes to evaluate spatial PAM technologies in a more controlled and repeatable manner.

We set three objectives to validate this approach:
* (O1) to determine whether the VSE could replicate natural soundscapes well enough to be a test environment;
* (O2) to pilot the VSE as a test environment for Passive Acoustic Monitoring (PAM) hardware; and, 
* (O3) to pilot the VSE as a test platform for PAM software.

To meet these objectives, we used a recently-developed, open source six-microphone field recorder to capture recordings of six field sites and their VSE-based simulations. Sites were based at the Imperial College Silwood Park Campus (Ascot, UK). For O1, we compared field and VSE recordings using a typical suite of ecoacoustic metrics. For O2, we used the VSE to explore how orientation impacts the performance of a six-microphone array. We extended the suite of metrics from O1 to compare VSE recordings from this array at various pitch angles: vertical (as in the field), 45° pitch, and horizontal. For O3, we investigate how BirdNET and HARKBird, software for classifying and localising avian calls, respectively, perform on bird calls added to the VSE-replicated soundscapes. We compare adding calls by encoding to the ambisonics domain and by playback from individual loudspeakers. 

The corresponding database with files needed for, and in some instances, generated by, the scripts in this repository are available on [Zenodo - peer review link](https://zenodo.org/records/10358602?token=eyJhbGciOiJIUzUxMiJ9.eyJpZCI6Ijc2ZmZhOWZkLTNlZjUtNGIxMC1hZGM2LTcwYjY4NTA5MmFlZiIsImRhdGEiOnt9LCJyYW5kb20iOiJiZDQ1NmJlNjdmZDQzMjlkOTlmMWI4ZjI3N2E0ZTIzNSJ9.6eXlik5JiWPqowqW74LcLVhv_a3urnpfm5N8iRnVf_4utoZFnOvJTKoIsuzn4EUZWgweRhjXkfHzXUgxsqf26w). 

## Repository Overview
The following scripts are included in this repository (further detail provided in comments in each script):
1. ``AddedBirdAnalaysis.m`` – this MATLAB script analyses the performance of BirdNET and HARKBird on additional bird calls for O3 and generates Fig. 7 and Fig. S2 in the accompanying article and supplementary material. This requires the files stored in ‘BirdNET O3 Outputs’ and ‘HARKBird O3 Outputs’ in the Zenodo database. Note that the six-channel recordings passed to HARKBird for this analysis were low-passed with a 4 kHz cutoff and 12 dB roll-off to avoid spatial aliasing. 
2. ``BirdNET_HARKBird_O1O2Stats.m`` – this MATLAB script analyses BirdNET and HARKBird for O1 and O2, and generates Figure 6 and the results featured in Tables 2 and 3. It requires the files stored in ‘BirdNET O1O2 Outputs’ and ‘HARKBird O1O2 Outputs’ in the Zenodo database.
3. ``ExtractAcousticIndices.R`` – this R script extracts 7 common Acoustic Indices for O1 and O2 using the first channel (corresponding to Mic 1) of the six-channel recordings in the folder ‘6mic Audio O1O2’ on Zenodo. Note that these recordings were also low-passed with a 4 kHz cutoff and 12 dB roll-off prior to this analysis. Also note the filenames used when importing the audio in the first part of the script. The outputs of this script are contained in the folder ‘Acoustic Indices’ on Zenodo.
4. ``ExtractVGGishEmbeddings.m`` – this MATLAB script extracts the 128-dimension feature embedding of the pre-trained VGGish convolutional neural network from the 6-mic recordings for O1 and O2. As with the Acoustic Indices, this requires the first channel of the six-channel recordings in the folder ‘6mic Audio O1O2’ on Zenodo, which we low-passed (again, 4 kHz cutoff and 12 dB roll-off) prior to analysis. Also note the filenames used when importing the audio in the first part of the script. The outputs of this script are contained in the folder ‘VGGish Features’ on Zenodo.
5. ``ScaledDiffs.m`` – this MATLAB script undertakes the Bland Altman analysis for O1 and O2 looking at scaled differences in Acoustic Indices’ and VGGish features’ values between the field and VSE-based re-recordings. This creates Fig. 4, Supp. Fig. 1, and results presented in Table 2 of the accompanying article. This script requires the data stored on Zenodo in the folders ‘Acoustic Indices’ and ‘VGGish Features’.
6. ``SpetroDiffPlot.m`` – this MATLAB script computes spectrograms of the 6-mic recordings for O1 and O2, and calculates the differences between the VSE-based spectrograms and field ones. The script produces Figs. 3 and 5 and the content for Table 1. It requires the multichannel recordings in the folder ‘6mic Audio O1O2’ on Zenodo. Note that the results presented in the article are also based on a version of these recordings with a low-pass filter (4 kHz cutoff frequency and 12 dB roll-off) applied to eliminate anomalous noise. 
